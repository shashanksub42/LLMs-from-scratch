{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Attention Mechanisms"
      ],
      "metadata": {
        "id": "j7vhp8UVoD7W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attending to different parts of the input with self-attention"
      ],
      "metadata": {
        "id": "HGTh9WYgoHvE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2-BPnLpwnOUU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Compute the unnormalized attention scores by computing the dot product between the query and all other input tokens\n",
        "\n",
        "query = inputs[1] # 2nd input token is the query\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, xi in enumerate(inputs):\n",
        "  attn_scores_2[i] = torch.dot(xi, query)\n",
        "\n",
        "attn_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oW2rr1GbohRW",
        "outputId": "fed8867a-9057-40dd-ef5a-16ff657a4260"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Normalize the unnormalized attention scores so that they sum up to 1\n",
        "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
        "\n",
        "print('Attention Weights: ', attn_weights_2_tmp)\n",
        "print('Sum: ', sum(attn_weights_2_tmp))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kj7J1id4pJ7t",
        "outputId": "6c84a1db-23e5-4a94-a3fc-b150b3751801"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:  tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "Sum:  tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using softmax for normalization is more efficient as it is better at handling extreme values\n",
        "\n",
        "def softmax_naive(x):\n",
        "  return torch.exp(x) / torch.exp(x).sum(dim=0)\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print('Attention Weights: ', attn_weights_2_naive)\n",
        "print('Sum: ', sum(attn_weights_2_naive))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQNdPiWaptYD",
        "outputId": "5efd78fe-848f-4629-aa6d-4bb0c1f5661a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum:  tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# However the naive implementation can suffer from numerical instabilities\n",
        "# Better to use the PyTorch implementation\n",
        "\n",
        "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "\n",
        "print('Attention Weights: ', attn_weights_2)\n",
        "print('Sum: ', sum(attn_weights_2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZmyZw08qDsT",
        "outputId": "ba201906-5029-431e-8c3b-c2d27df81c03"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Weights:  tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "Sum:  tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Comput the context vector z2\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i, xi in enumerate(inputs):\n",
        "  context_vec_2 += attn_weights_2[i]*xi\n",
        "\n",
        "print(context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQwfwouSqT3n",
        "outputId": "47a3db4c-7153-4055-a87b-9c01fe6308ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Computing attention weights for all input tokens"
      ],
      "metadata": {
        "id": "nVmUlUlErCGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores = torch.empty(6,6)\n",
        "\n",
        "for i, xi in enumerate(inputs):\n",
        "  for j, xj in enumerate(inputs):\n",
        "    attn_scores[i,j] = torch.dot(xi, xj)\n",
        "\n",
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIMeogW0qr7c",
        "outputId": "b3f55484-16a7-49c6-80b0-57f698a100fe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
              "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
              "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
              "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
              "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
              "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can achieve the same as above more efficiently via matrix multiplication:\n",
        "attn_scores = inputs @ inputs.T\n",
        "attn_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cF1tYuNsEh2",
        "outputId": "9cb154fb-2f1f-4a07-d2e2-08d65f2f6b7e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
              "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
              "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
              "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
              "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
              "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2huxrQQsLm0",
        "outputId": "cb60b629-5333-48bd-fda7-616782e8b322"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
              "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
              "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
              "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
              "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
              "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_context_vectors = attn_weights @ inputs\n",
        "all_context_vectors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44ul0psmsV1F",
        "outputId": "20b243d8-2baa-4796-fe6a-288ee794d87a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.4421, 0.5931, 0.5790],\n",
              "        [0.4419, 0.6515, 0.5683],\n",
              "        [0.4431, 0.6496, 0.5671],\n",
              "        [0.4304, 0.6298, 0.5510],\n",
              "        [0.4671, 0.5910, 0.5266],\n",
              "        [0.4177, 0.6503, 0.5645]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Previous 2nd context vector:\", context_vec_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DJDhxDssevY",
        "outputId": "d5165891-0cc9-4cb1-df39-9c408181eaef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing self-attention with trainable weights\n",
        "\n",
        "- Compute context vectors as weighted sums over the input vectors specific to a certain input element\n",
        "- For the above, we need attention weights\n",
        "- The most notable difference between this technique and the more simplified way to produce context vectors presented above is the introduction of weight matricies which will be updated during training"
      ],
      "metadata": {
        "id": "R1eGOtcesl0U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computing the attention weights step by step\n",
        "\n",
        "- Implement the self-attention mechanism by introducing the three weight matricies: Wq, Wk, Wv\n",
        "- These three matricies are used to project the input tokens into query, key and value vectors via matrix multiplication\n",
        "\n",
        "- Query vector: $q^{(i)} = W_{q}x^{(i)}$\n",
        "- Key vector: $k^{(i)} = W_{k}x^{(i)}$\n",
        "- Value vector: $v^{(i)} = W_{v}x^{(i)}$"
      ],
      "metadata": {
        "id": "pslsHC-Ls8Jn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Implement the self-attention mechanism by introducing the three weight matricies: Wq, Wk, Wv\n",
        "# These three matricies are used to project the input tokens into query, key and value vectors via matrix multiplication\n",
        "\n",
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1] # the input embedding size, d=3\n",
        "d_out = 2 # the output embedding size, d=2"
      ],
      "metadata": {
        "id": "IRGKmcu9skSs"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the three weight matricies\n",
        "\n",
        "W_query = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_key = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)\n",
        "W_value = torch.nn.Parameter(torch.rand(d_in, d_out), requires_grad=False)"
      ],
      "metadata": {
        "id": "2759b8zku_uT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Comput the query, key and value vectors\n",
        "\n",
        "query_2 = x_2 @ W_query\n",
        "key_2 = x_2 @ W_key\n",
        "value_2 = x_2 @ W_value\n",
        "\n",
        "print(query_2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7K1sXz8vISE",
        "outputId": "553d87c6-108e-4d39-cf00-95cf0a39da46"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.8705, 1.2787])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "keys = inputs @ W_key\n",
        "values = inputs @ W_value\n",
        "\n",
        "keys.shape, values.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrjR9Vb_vW7q",
        "outputId": "6c69802e-4c9f-4c6e-92a3-6e636dfb3615"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([6, 2]), torch.Size([6, 2]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Comput the unnnormalized attention scores by computing the dot product between the query and each key vector\n",
        "\n",
        "keys_2 = keys[1]\n",
        "attn_score_22 = query_2.dot(keys_2)\n",
        "attn_score_22"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4K_g_7BvuY0",
        "outputId": "c7d3b2e5-81a8-4a56-eb74-86dfe963de0f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.7332)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_scores_2 = query_2 @ keys.T # All attention scores for a given query\n",
        "attn_scores_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-oWn-vMwBNj",
        "outputId": "a4bd67da-ad4b-44e2-e437-1a3432a1212c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.4664, 3.7332, 3.6982, 2.0531, 2.0264, 2.5277])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, we compute the attention weights using the softmax function\n",
        "# Only difference is that we now scale the attention scores by dividing them by the square root of the embedding dimension\n",
        "\n",
        "d_k = keys.shape[1]\n",
        "attn_weights_2 = torch.softmax(attn_scores_2 / d_k**0.5, dim=-1)\n",
        "attn_weights_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y9awudprwM4f",
        "outputId": "83c68e86-8936-4270-d303-83f7a2302a48"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1196, 0.2929, 0.2857, 0.0893, 0.0876, 0.1249])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally we compute the context vector of input query vector 2\n",
        "\n",
        "context_vec_2 = attn_weights_2 @ values\n",
        "context_vec_2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XoSYvyQjwsIm",
        "outputId": "b4d84b9d-5923-4366-acb5-756eae85bd36"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7125, 0.8965])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing a compact self-attention class"
      ],
      "metadata": {
        "id": "fhT_VBGCxInd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "  def __init__(self, d_in, d_out):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_key = nn.Parameter(torch.rand(d_in, d_out))\n",
        "    self.W_value = nn.Parameter(torch.rand(d_in, d_out))\n",
        "\n",
        "  def forward(self, X):\n",
        "    keys = X @ self.W_key\n",
        "    queries = X @ self.W_query\n",
        "    values = X @ self.W_value\n",
        "\n",
        "    attn_scores = queries @ keys.T #omega\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "fLUHY2B-w2gj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4SvOO3wex24v",
        "outputId": "7df65965-3e4c-4b22-f560-a45273f2b663"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can streamline the implementation above using PyTorch's Linear layers. This is equivalent to a matrix multiplication.\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, X):\n",
        "    keys = self.W_key(X)\n",
        "    queries = self.W_query(X)\n",
        "    values = self.W_value(X)\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "Pq4jyfq0x-dv"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRyTwm9wy0N0",
        "outputId": "16928c0e-bacc-4f6a-f0e4-b3b0de59ccad"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.5337, -0.1051],\n",
            "        [-0.5323, -0.1080],\n",
            "        [-0.5323, -0.1079],\n",
            "        [-0.5297, -0.1076],\n",
            "        [-0.5311, -0.1066],\n",
            "        [-0.5299, -0.1081]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hiding future weights with causal attention\n",
        "\n",
        "In causal attention, the attention weights above the diagonal are masked, ensuring that for any given input, the LLM is unable to utilize future tokens while calculating the context vectors with the attention weight"
      ],
      "metadata": {
        "id": "XHNBgSVUzAJA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying a causal attention mask\n",
        "\n",
        "1. Get attention scores\n",
        "2. Apply softmax\n",
        "3. Max with 0s above diagonal\n",
        "4. Masked attention scores (unnormalized)\n",
        "5. Normalize rows to get masked attention weights"
      ],
      "metadata": {
        "id": "1HAoNel40Rwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "values = sa_v2.W_value(inputs)\n",
        "\n",
        "attn_scores = queries @ keys.T\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CiInmJS1y2nD",
        "outputId": "3d121465-3286-480f-ce83-02c8f30c0ca4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1717, 0.1762, 0.1761, 0.1555, 0.1627, 0.1579],\n",
              "        [0.1636, 0.1749, 0.1746, 0.1612, 0.1605, 0.1652],\n",
              "        [0.1637, 0.1749, 0.1746, 0.1611, 0.1606, 0.1651],\n",
              "        [0.1636, 0.1704, 0.1702, 0.1652, 0.1632, 0.1674],\n",
              "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.1639],\n",
              "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The simplest way to mask out future attention weights is by creating a mask via PyTorch's tril function with elements below the main diagonal (including the diagonal itself) set to 1 and above the main diagonal set to 0"
      ],
      "metadata": {
        "id": "VQVzpzmd1D7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "masked_simple = torch.tril(torch.ones(context_length, context_length))\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWWUlqZe0_c9",
        "outputId": "66b6b5da-4d77-4e61-8824-b169bbc17cd4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights*masked_simple\n",
        "masked_simple"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJpdUhJe1Oo-",
        "outputId": "87e34652-d7bc-42f2-aa7f-78833ae6ca9b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.1717, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1636, 0.1749, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1637, 0.1749, 0.1746, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1636, 0.1704, 0.1702, 0.1652, 0.0000, 0.0000],\n",
              "        [0.1667, 0.1722, 0.1721, 0.1618, 0.1633, 0.0000],\n",
              "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
              "       grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- However, if the mask were applied after softmax, like above, it would disrupt the probability distribution created by softmax\n",
        "- Softmax ensures that all output values sum to 1\n",
        "- Masking after softmax would require re-normalizing the outputs to sum to 1 again, which complicates the process and might lead to unintended effects"
      ],
      "metadata": {
        "id": "svNsDlqO1tV-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "row_sums = masked_simple.sum(dim=-1, keepdim=True)\n",
        "masked_simple_norm = masked_simple / row_sums\n",
        "masked_simple_norm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yqupyib1s6e",
        "outputId": "a5030478-962e-443f-d9a3-b6eb641c0776"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
              "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
              "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
              "       grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "masked"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXClhR9y1VMV",
        "outputId": "eeb8c287-77d5-4bae-b300-ae75a1678c18"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3111,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.1655, 0.2602,   -inf,   -inf,   -inf,   -inf],\n",
              "        [0.1667, 0.2602, 0.2577,   -inf,   -inf,   -inf],\n",
              "        [0.0510, 0.1080, 0.1064, 0.0643,   -inf,   -inf],\n",
              "        [0.1415, 0.1875, 0.1863, 0.0987, 0.1121,   -inf],\n",
              "        [0.0476, 0.1192, 0.1171, 0.0731, 0.0477, 0.0966]],\n",
              "       grad_fn=<MaskedFillBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked/keys.shape[-1]**0.5, dim=-1)\n",
        "attn_weights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDGv-Mxf2ENg",
        "outputId": "41b09221-36de-4d80-9a0e-d5e5ed76630a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.4833, 0.5167, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.3190, 0.3408, 0.3402, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2445, 0.2545, 0.2542, 0.2468, 0.0000, 0.0000],\n",
              "        [0.1994, 0.2060, 0.2058, 0.1935, 0.1953, 0.0000],\n",
              "        [0.1624, 0.1709, 0.1706, 0.1654, 0.1625, 0.1682]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masking additional weights with dropout\n",
        "\n",
        "- In addition, we also apply dropout to reduce overfitting during training\n",
        "- We will apply the dropout mask after computing the attention weights because it's more common\n",
        "- Furthermore, in this specific example, we use a dropout rate of 50%, which means randomly masking out half of the attention weights. (When we train the GPT model later, we will use a lower dropout rate, such as 0.1 or 0.2)\n",
        "- If we apply a dropout rate of 0.5 (50%), the non-dropped values will be scaled accordingly by a factor of 1/0.5 = 2\n",
        "- The scaling is calculated by the formula `1 / (1 - dropout_rate)`\n"
      ],
      "metadata": {
        "id": "SV7rT9d_2QhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = torch.nn.Dropout(0.5) # dropout rate of 50%\n",
        "example = torch.ones(6,6)\n",
        "\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TrUVsQM2LdI",
        "outputId": "e36936e3-89c2-407a-93ed-b3bc36df0ee3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG5vIrm82xCk",
        "outputId": "51b55470-440b-43e4-ce18-c7e9ddffc4e1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0335, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6804, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4889, 0.5090, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3988, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3418, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing a compact causal self-attention class\n",
        "\n",
        "- Now, we are ready to implement a working implementation of self-attention, including the causal and dropout masks\n",
        "- One more thing is to implement the code to handle batches consisting of more than one input so that our CausalAttention class supports the batch outputs produced by the data loader we implemented in chapter 2\n",
        "- For simplicity, to simulate such batch input, we duplicate the input text example:"
      ],
      "metadata": {
        "id": "V4Llx3vp27Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape) # 2 inputs with 6 tokens each, and each token has embedding dimension 3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlFZtcml22s6",
        "outputId": "2b3f844b-ac2f-459a-b19f-c718ac34df6b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, X):\n",
        "    b, num_tokens, d_in = X.shape\n",
        "    keys = self.W_key(X)\n",
        "    queries = self.W_query(X)\n",
        "    values = self.W_value(X)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1,2)\n",
        "    attn_scores.masked_fill_(\n",
        "        self.mask.bool()[:num_tokens, :num_tokens], -torch.inf\n",
        "    )\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "    )\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "yrEu9R_j3Jz6"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "context_vecs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1ttC8er4fMe",
        "outputId": "94dc7c81-2cf3-415c-e87c-274f787b167f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 6, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extending single-head attention to multi-head attention"
      ],
      "metadata": {
        "id": "CQ6I5kki5Wfb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stacking multiple single-head attention layers\n",
        "\n",
        "The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections. This allows the model to jointly attend to information from different representation subspaces at different positions."
      ],
      "metadata": {
        "id": "ys7SaOxu5aeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList(\n",
        "        [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias) for _ in range(num_heads)]\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    return torch.cat([head(X) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "ryKnLtrC4t6g"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3,2\n",
        "\n",
        "mha = MultiHeadAttentionWrapper(d_in, d_out, context_length, dropout=0.0, num_heads=2)\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8qxU82snrCKi",
        "outputId": "46e5125e-742d-40fe-be99-b1436a53b7fa"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "torch.Size([2, 6, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the implementation above, the embedding dimension is 4, because we d_out=2 as the embedding dimension for the key, query, and value vectors as well as the context vector. And since we have 2 attention heads, we have the output embedding dimension 2*2=4"
      ],
      "metadata": {
        "id": "dUgoqj8Hr5Ew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing multi-head attention with weight splits\n",
        "\n",
        "- While the above is an intuitive and fully functional implementation of multi-head attention (wrapping the single-head attention CausalAttention implementation from earlier), we can write a stand-alone class called MultiHeadAttention to achieve the same\n",
        "\n",
        "- We don't concatenate single attention heads for this stand-alone MultiHeadAttention class\n",
        "\n",
        "- Instead, we create single W_query, W_key, and W_value weight matrices and then split those into individual matrices for each attention head:"
      ],
      "metadata": {
        "id": "cwTPLbfCr7hd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
        "\n",
        "    self.d_out = d_out\n",
        "    self.num_heads = num_heads\n",
        "    self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
        "\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "    self.out_proj = nn.Linear(d_out, d_out) # Linear layer to combine head outputs\n",
        "\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    self.register_buffer(\n",
        "        'mask',\n",
        "        torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "    )\n",
        "\n",
        "  def forward(self, X):\n",
        "    b, num_tokens, d_in = X.shape\n",
        "\n",
        "    keys = self.W_key(X) # Shape: (b, num_tokens, d_out)\n",
        "    queries = self.W_query(X)\n",
        "    values = self.W_value(X)\n",
        "\n",
        "    # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "    # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "    keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "    values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "    # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "    keys = keys.transpose(1,2)\n",
        "    queries = queries.transpose(1,2)\n",
        "    values = values.transpose(1,2)\n",
        "\n",
        "    # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "    attn_scores = queries @ keys.transpose(2,3) # Dot product for each head\n",
        "\n",
        "    # Original mask truncated to the number of tokens and converted to boolean\n",
        "    mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "    # Use the mask to fill attention scores\n",
        "    attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "    # Normalize using softmax\n",
        "    attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "    # Add dropout\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "    context_vec = (attn_weights @ values).transpose(1,2)\n",
        "\n",
        "    # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "    context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "    context_vec = self.out_proj(context_vec)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "Mnx8_wGlrqym"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, num_heads=2, dropout=0.0)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "print(context_vecs)\n",
        "print(context_vecs.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p982JUhIuY6Z",
        "outputId": "79072383-dfe5-482e-f152-ac10753c1fb4"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]],\n",
            "\n",
            "        [[0.3190, 0.4858],\n",
            "         [0.2943, 0.3897],\n",
            "         [0.2856, 0.3593],\n",
            "         [0.2693, 0.3873],\n",
            "         [0.2639, 0.3928],\n",
            "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n",
            "torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that in addition, we added a linear projection layer (self.out_proj ) to the MultiHeadAttention class above. This is simply a linear transformation that doesn't change the dimensions. It's a standard convention to use such a projection layer in LLM implementation, but it's not strictly necessary (recent research has shown that it can be removed without affecting the modeling performance; see the further reading section at the end of this chapter)"
      ],
      "metadata": {
        "id": "NHdT86CvxymJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Notes on how the transpose works\n",
        "# attn_scores = queries @ keys.transpose(2,3)\n",
        "\n",
        "# (b, num_heads, num_tokens, head_dim) = (1, 2, 3, 4)\n",
        "a = torch.tensor([[[[0.2745, 0.6584, 0.2775, 0.8573],\n",
        "                    [0.8993, 0.0390, 0.9268, 0.7388],\n",
        "                    [0.7179, 0.7058, 0.9156, 0.4340]],\n",
        "\n",
        "                   [[0.0772, 0.3565, 0.1479, 0.5331],\n",
        "                    [0.4066, 0.2318, 0.4545, 0.9737],\n",
        "                    [0.4606, 0.5159, 0.4220, 0.5786]]]])\n",
        "\n",
        "print(a @ a.transpose(2, 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SsmWzN1ixq33",
        "outputId": "a697de77-9b5a-499d-c359-6bf2044ba3a6"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[1.3208, 1.1631, 1.2879],\n",
            "          [1.1631, 2.2150, 1.8424],\n",
            "          [1.2879, 1.8424, 2.0402]],\n",
            "\n",
            "         [[0.4391, 0.7003, 0.5903],\n",
            "          [0.7003, 1.3737, 1.0620],\n",
            "          [0.5903, 1.0620, 0.9912]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, the matrix multiplication implementation in PyTorch will handle the 4-dimensional input tensor so that the matrix multiplication is carried out between the 2 last dimensions (num_tokens, head_dim) and then repeated for the individual heads"
      ],
      "metadata": {
        "id": "39jFJl_oyiTw"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TjJlxGM2yfzm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}